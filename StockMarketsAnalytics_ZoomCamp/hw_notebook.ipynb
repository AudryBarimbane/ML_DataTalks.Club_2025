{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EJE7jpA7ymzl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "ec_5htssy8xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T√©l√©charger les donn√©es S&P 500 depuis Wikipedia\n",
        "print(\"üìä T√©l√©chargement des donn√©es S&P 500 depuis Wikipedia...\")\n",
        "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "\n",
        "tables = pd.read_html(url)\n",
        "sp500_df = tables[0]\n",
        "\n",
        "print(f\"‚úÖ Donn√©es t√©l√©charg√©es avec succ√®s!\")\n",
        "print(f\"üìà Nombre d'entreprises: {len(sp500_df)}\")\n",
        "print(f\"üìã Colonnes disponibles: {list(sp500_df.columns)}\")\n",
        "\n",
        "# Identifier la colonne contenant les dates d'ajout\n",
        "date_column = None\n",
        "for col in sp500_df.columns:\n",
        "    if 'date' in col.lower() or 'added' in col.lower():\n",
        "        date_column = col\n",
        "        break\n",
        "\n",
        "# Fonction pour extraire l'ann√©e\n",
        "def extract_year(date_str):\n",
        "    if pd.isna(date_str) or date_str == '':\n",
        "        return None\n",
        "\n",
        "    date_str = str(date_str).strip()\n",
        "\n",
        "    try:\n",
        "        # Format YYYY-MM-DD\n",
        "        if '-' in date_str and len(date_str.split('-')[0]) == 4:\n",
        "            return int(date_str.split('-')[0])\n",
        "        # Format MM/DD/YYYY\n",
        "        elif '/' in date_str:\n",
        "            parts = date_str.split('/')\n",
        "            if len(parts) == 3 and len(parts[2]) == 4:\n",
        "                return int(parts[2])\n",
        "        # Format YYYY seulement\n",
        "        elif date_str.isdigit() and len(date_str) == 4:\n",
        "            return int(date_str)\n",
        "        # Autres formats avec pandas\n",
        "        else:\n",
        "            parsed_date = pd.to_datetime(date_str, errors='coerce')\n",
        "            if not pd.isna(parsed_date):\n",
        "                return parsed_date.year\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return None\n",
        "\n",
        "# Extraire les ann√©es d'ajout\n",
        "sp500_df['Year_Added'] = sp500_df[date_column].apply(extract_year)\n",
        "\n",
        "# Filtrer les donn√©es valides (exclure 1957 et les valeurs nulles)\n",
        "valid_years = sp500_df.dropna(subset=['Year_Added'])\n",
        "valid_years = valid_years[valid_years['Year_Added'] != 1957]\n",
        "\n",
        "print(f\"üìä Entreprises avec ann√©es d'ajout valides: {len(valid_years)}\")\n",
        "\n",
        "# Calculer le nombre d'ajouts par ann√©e\n",
        "yearly_additions = valid_years['Year_Added'].value_counts().sort_index()\n",
        "\n",
        "# Trouver l'ann√©e avec le plus d'ajouts\n",
        "max_additions_year = yearly_additions.idxmax()\n",
        "max_additions_count = yearly_additions.max()\n",
        "\n",
        "print(f\"\\nüèÜ R√âPONSE √Ä LA QUESTION:\")\n",
        "print(f\"L'ann√©e avec le plus d'ajouts: {int(max_additions_year)}\")\n",
        "print(f\"Nombre d'ajouts cette ann√©e-l√†: {max_additions_count}\")\n",
        "\n",
        "# V√©rifier s'il y a plusieurs ann√©es avec le m√™me maximum\n",
        "years_with_max = yearly_additions[yearly_additions == max_additions_count]\n",
        "if len(years_with_max) > 1:\n",
        "    most_recent_max_year = years_with_max.index.max()\n",
        "    print(f\"Plusieurs ann√©es ont {max_additions_count} ajouts.\")\n",
        "    print(f\"L'ann√©e la plus r√©cente avec le maximum: {int(most_recent_max_year)}\")\n",
        "\n",
        "# Question additionnelle: Entreprises pr√©sentes depuis plus de 20 ans\n",
        "current_year = 2025\n",
        "cutoff_year = current_year - 20\n",
        "companies_20_plus_years = valid_years[valid_years['Year_Added'] <= cutoff_year]\n",
        "\n",
        "print(f\"\\nüìÖ QUESTION ADDITIONNELLE:\")\n",
        "print(f\"Entreprises pr√©sentes depuis plus de 20 ans (ajout√©es avant {cutoff_year}): {len(companies_20_plus_years)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHduy64Zy4mV",
        "outputId": "40cc1efe-441d-4776-e3bd-2fd7607e5f36"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä T√©l√©chargement des donn√©es S&P 500 depuis Wikipedia...\n",
            "‚úÖ Donn√©es t√©l√©charg√©es avec succ√®s!\n",
            "üìà Nombre d'entreprises: 503\n",
            "üìã Colonnes disponibles: ['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry', 'Headquarters Location', 'Date added', 'CIK', 'Founded']\n",
            "üìä Entreprises avec ann√©es d'ajout valides: 450\n",
            "\n",
            "üèÜ R√âPONSE √Ä LA QUESTION:\n",
            "L'ann√©e avec le plus d'ajouts: 2016\n",
            "Nombre d'ajouts cette ann√©e-l√†: 23\n",
            "Plusieurs ann√©es ont 23 ajouts.\n",
            "L'ann√©e la plus r√©cente avec le maximum: 2017\n",
            "\n",
            "üìÖ QUESTION ADDITIONNELLE:\n",
            "Entreprises pr√©sentes depuis plus de 20 ans (ajout√©es avant 2005): 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2"
      ],
      "metadata": {
        "id": "H9T00jDVzBwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üåç ANALYSE DES INDICES BOURSIERS MONDIAUX - YTD 2025\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# D√©finition des indices avec leurs symboles Yahoo Finance\n",
        "indices = {\n",
        "    'United States': '^GSPC',        # S&P 500\n",
        "    'China': '000001.SS',            # Shanghai Composite\n",
        "    'Hong Kong': '^HSI',             # Hang Seng Index\n",
        "    'Australia': '^AXJO',            # S&P/ASX 200\n",
        "    'India': '^NSEI',                # Nifty 50\n",
        "    'Canada': '^GSPTSE',             # S&P/TSX Composite\n",
        "    'Germany': '^GDAXI',             # DAX\n",
        "    'United Kingdom': '^FTSE',       # FTSE 100\n",
        "    'Japan': '^N225',                # Nikkei 225\n",
        "    'Mexico': '^MXX',                # IPC Mexico\n",
        "    'Brazil': '^BVSP'                # Ibovespa\n",
        "}\n",
        "\n",
        "# Param√®tres de date pour YTD 2025\n",
        "start_date = '2025-01-01'\n",
        "end_date = '2025-05-01'\n",
        "print(f\"üìÖ P√©riode d'analyse: {start_date} √† {end_date}\")\n",
        "print(f\"üìä Nombre d'indices analys√©s: {len(indices)}\")\n",
        "\n",
        "# Fonction optimis√©e pour t√©l√©charger les donn√©es\n",
        "def get_ytd_performance(symbol, start_date, end_date):\n",
        "    \"\"\"T√©l√©charge les donn√©es et calcule la performance YTD\"\"\"\n",
        "    try:\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        data = ticker.history(start=start_date, end=end_date, auto_adjust=True)\n",
        "\n",
        "        if data.empty:\n",
        "            return None\n",
        "\n",
        "        first_price = data['Close'].iloc[0]\n",
        "        last_price = data['Close'].iloc[-1]\n",
        "        ytd_return = ((last_price - first_price) / first_price) * 100\n",
        "\n",
        "        return {\n",
        "            'Symbol': symbol,\n",
        "            'Start_Price': first_price,\n",
        "            'End_Price': last_price,\n",
        "            'YTD_Return_%': ytd_return\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur pour {symbol}: {str(e)[:50]}...\")\n",
        "        return None\n",
        "\n",
        "# T√©l√©chargement des donn√©es\n",
        "print(f\"\\nüìà T√©l√©chargement des donn√©es...\")\n",
        "detailed_data = {}\n",
        "\n",
        "for country, symbol in indices.items():\n",
        "    print(f\"   T√©l√©chargement: {country} ({symbol})\")\n",
        "    result = get_ytd_performance(symbol, start_date, end_date)\n",
        "\n",
        "    if result:\n",
        "        detailed_data[country] = result\n",
        "        print(f\"   ‚úÖ {country}: {result['YTD_Return_%']:+.2f}%\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå √âchec pour {country}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Donn√©es t√©l√©charg√©es avec succ√®s pour {len(detailed_data)}/{len(indices)} indices\")\n",
        "\n",
        "# Cr√©ation du DataFrame et analyse\n",
        "if detailed_data:\n",
        "    df_results = pd.DataFrame.from_dict(detailed_data, orient='index')\n",
        "    df_results = df_results.sort_values('YTD_Return_%', ascending=False)\n",
        "\n",
        "    # Affichage des r√©sultats YTD\n",
        "    print(f\"\\nüèÜ PERFORMANCES YTD (1 Jan - 1 Mai 2025):\")\n",
        "    print(\"=\" * 60)\n",
        "    for i, (country, row) in enumerate(df_results.iterrows(), 1):\n",
        "        symbol = row['Symbol']\n",
        "        ytd_return = row['YTD_Return_%']\n",
        "        status = \"üìà\" if ytd_return > 0 else \"üìâ\"\n",
        "        print(f\"{i:2d}. {country:<15} ({symbol:<10}): {ytd_return:+7.2f}% {status}\")\n",
        "\n",
        "    # R√©ponse √† la question principale\n",
        "    if 'United States' in detailed_data:\n",
        "        sp500_return = detailed_data['United States']['YTD_Return_%']\n",
        "        better_than_sp500 = sum(1 for country, data in detailed_data.items()\n",
        "                              if data['YTD_Return_%'] > sp500_return and country != 'United States')\n",
        "\n",
        "        print(f\"\\nüéØ R√âPONSE √Ä LA QUESTION:\")\n",
        "        print(f\"S&P 500 (√âtats-Unis) YTD: {sp500_return:+.2f}%\")\n",
        "        print(f\"Indices avec de meilleures performances: {better_than_sp500} sur {len(detailed_data)-1}\")\n",
        "\n",
        "    # Statistiques suppl√©mentaires\n",
        "    returns = [data['YTD_Return_%'] for data in detailed_data.values()]\n",
        "    positive_returns = sum(1 for ret in returns if ret > 0)\n",
        "    negative_returns = len(returns) - positive_returns\n",
        "    avg_return = np.mean(returns)\n",
        "\n",
        "    print(f\"\\nüìä STATISTIQUES SUPPL√âMENTAIRES:\")\n",
        "    print(f\"Rendements positifs: {positive_returns}/{len(returns)}\")\n",
        "    print(f\"Rendements n√©gatifs: {negative_returns}/{len(returns)}\")\n",
        "    print(f\"Rendement moyen: {avg_return:+.2f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ùå Aucune donn√©e n'a pu √™tre t√©l√©charg√©e. V√©rifiez votre connexion internet.\")\n",
        "\n",
        "print(f\"\\nüîö Analyse termin√©e!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_8Zz2dvzGqs",
        "outputId": "3d18c10a-bbc5-4418-e6a6-1257ed970e85"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üåç ANALYSE DES INDICES BOURSIERS MONDIAUX - YTD 2025\n",
            "================================================================================\n",
            "üìÖ P√©riode d'analyse: 2025-01-01 √† 2025-05-01\n",
            "üìä Nombre d'indices analys√©s: 11\n",
            "\n",
            "üìà T√©l√©chargement des donn√©es...\n",
            "   T√©l√©chargement: United States (^GSPC)\n",
            "   ‚úÖ United States: -5.10%\n",
            "   T√©l√©chargement: China (000001.SS)\n",
            "   ‚úÖ China: +0.50%\n",
            "   T√©l√©chargement: Hong Kong (^HSI)\n",
            "   ‚úÖ Hong Kong: +12.72%\n",
            "   T√©l√©chargement: Australia (^AXJO)\n",
            "   ‚úÖ Australia: -0.91%\n",
            "   T√©l√©chargement: India (^NSEI)\n",
            "   ‚úÖ India: +2.49%\n",
            "   T√©l√©chargement: Canada (^GSPTSE)\n",
            "   ‚úÖ Canada: -0.23%\n",
            "   T√©l√©chargement: Germany (^GDAXI)\n",
            "   ‚úÖ Germany: +12.35%\n",
            "   T√©l√©chargement: United Kingdom (^FTSE)\n",
            "   ‚úÖ United Kingdom: +2.84%\n",
            "   T√©l√©chargement: Japan (^N225)\n",
            "   ‚úÖ Japan: -8.30%\n",
            "   T√©l√©chargement: Mexico (^MXX)\n",
            "   ‚úÖ Mexico: +13.05%\n",
            "   T√©l√©chargement: Brazil (^BVSP)\n",
            "   ‚úÖ Brazil: +12.44%\n",
            "\n",
            "‚úÖ Donn√©es t√©l√©charg√©es avec succ√®s pour 11/11 indices\n",
            "\n",
            "üèÜ PERFORMANCES YTD (1 Jan - 1 Mai 2025):\n",
            "============================================================\n",
            " 1. Mexico          (^MXX      ):  +13.05% üìà\n",
            " 2. Hong Kong       (^HSI      ):  +12.72% üìà\n",
            " 3. Brazil          (^BVSP     ):  +12.44% üìà\n",
            " 4. Germany         (^GDAXI    ):  +12.35% üìà\n",
            " 5. United Kingdom  (^FTSE     ):   +2.84% üìà\n",
            " 6. India           (^NSEI     ):   +2.49% üìà\n",
            " 7. China           (000001.SS ):   +0.50% üìà\n",
            " 8. Canada          (^GSPTSE   ):   -0.23% üìâ\n",
            " 9. Australia       (^AXJO     ):   -0.91% üìâ\n",
            "10. United States   (^GSPC     ):   -5.10% üìâ\n",
            "11. Japan           (^N225     ):   -8.30% üìâ\n",
            "\n",
            "üéØ R√âPONSE √Ä LA QUESTION:\n",
            "S&P 500 (√âtats-Unis) YTD: -5.10%\n",
            "Indices avec de meilleures performances: 9 sur 10\n",
            "\n",
            "üìä STATISTIQUES SUPPL√âMENTAIRES:\n",
            "Rendements positifs: 7/11\n",
            "Rendements n√©gatifs: 4/11\n",
            "Rendement moyen: +3.80%\n",
            "\n",
            "üîö Analyse termin√©e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìâ QUESTION 3: ANALYSE DES CORRECTIONS DU S&P 500\")\n",
        "print(\"=\" * 80)\n",
        "print(\"üéØ Objectif: Calculer la dur√©e m√©diane des corrections significatives (>5%)\")\n",
        "\n",
        "# T√©l√©chargement des donn√©es historiques S&P 500 depuis 1950\n",
        "print(\"\\nüìä T√©l√©chargement des donn√©es historiques S&P 500 (1950-pr√©sent)...\")\n",
        "start_date_historical = '1950-01-01'\n",
        "end_date_historical = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "# Fonction optimis√©e pour t√©l√©charger les donn√©es\n",
        "def download_sp500_historical(start_date, end_date):\n",
        "    \"\"\"T√©l√©charge les donn√©es historiques du S&P 500\"\"\"\n",
        "    try:\n",
        "        ticker = yf.Ticker('^GSPC')\n",
        "        data = ticker.history(start=start_date, end=end_date, auto_adjust=True)\n",
        "\n",
        "        if not data.empty:\n",
        "            print(f\"‚úÖ Donn√©es t√©l√©charg√©es: {len(data)} jours de {data.index[0].date()} √† {data.index[-1].date()}\")\n",
        "            return data\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Aucune donn√©e obtenue\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur t√©l√©chargement: {str(e)[:100]}...\")\n",
        "        return None\n",
        "\n",
        "# T√©l√©chargement des donn√©es\n",
        "sp500_data = download_sp500_historical(start_date_historical, end_date_historical)\n",
        "\n",
        "if sp500_data is None or sp500_data.empty:\n",
        "    print(\"‚ùå Impossible de t√©l√©charger les donn√©es S&P 500\")\n",
        "    exit()\n",
        "\n",
        "print(f\"üìà P√©riode analys√©e: {sp500_data.index[0].date()} √† {sp500_data.index[-1].date()}\")\n",
        "print(f\"üìä Nombre de points de donn√©es: {len(sp500_data)}\")\n",
        "\n",
        "# Identifier les sommets historiques (all-time highs)\n",
        "print(\"\\nüîç Identification des sommets historiques...\")\n",
        "sp500_data['Cumulative_Max'] = sp500_data['Close'].expanding().max()\n",
        "sp500_data['Is_ATH'] = sp500_data['Close'] == sp500_data['Cumulative_Max']\n",
        "\n",
        "ath_dates = sp500_data[sp500_data['Is_ATH']].index.tolist()\n",
        "print(f\"üìà Nombre de sommets historiques identifi√©s: {len(ath_dates)}\")\n",
        "\n",
        "# Analyser les corrections entre les ATH cons√©cutifs\n",
        "print(\"\\nüìâ Analyse des corrections entre les sommets...\")\n",
        "corrections = []\n",
        "\n",
        "for i in range(len(ath_dates) - 1):\n",
        "    start_date = ath_dates[i]\n",
        "    end_date = ath_dates[i + 1]\n",
        "\n",
        "    period_data = sp500_data.loc[start_date:end_date]\n",
        "\n",
        "    if len(period_data) < 2:\n",
        "        continue\n",
        "\n",
        "    high_price = period_data['Close'].iloc[0]\n",
        "    min_price = period_data['Close'].min()\n",
        "    min_date = period_data['Close'].idxmin()\n",
        "\n",
        "    drawdown_pct = ((high_price - min_price) / high_price) * 100\n",
        "    duration_days = (min_date - start_date).days\n",
        "\n",
        "    corrections.append({\n",
        "        'start_date': start_date,\n",
        "        'end_date': min_date,\n",
        "        'high_price': high_price,\n",
        "        'low_price': min_price,\n",
        "        'drawdown_pct': drawdown_pct,\n",
        "        'duration_days': duration_days\n",
        "    })\n",
        "\n",
        "print(f\"üìä Nombre total de p√©riodes analys√©es: {len(corrections)}\")\n",
        "\n",
        "# Filtrer les corrections significatives (>5%)\n",
        "significant_corrections = [c for c in corrections if c['drawdown_pct'] >= 5.0]\n",
        "print(f\"üìâ Corrections significatives (‚â•5%): {len(significant_corrections)}\")\n",
        "\n",
        "if not significant_corrections:\n",
        "    print(\"‚ùå Aucune correction significative trouv√©e dans les donn√©es\")\n",
        "    exit()\n",
        "\n",
        "# Cr√©er un DataFrame pour l'analyse\n",
        "corrections_df = pd.DataFrame(significant_corrections)\n",
        "corrections_df = corrections_df.sort_values('drawdown_pct', ascending=False)\n",
        "\n",
        "# Calculer les statistiques des dur√©es\n",
        "durations = [c['duration_days'] for c in significant_corrections]\n",
        "\n",
        "percentile_25 = np.percentile(durations, 25)\n",
        "percentile_50 = np.percentile(durations, 50)  # M√©diane\n",
        "percentile_75 = np.percentile(durations, 75)\n",
        "mean_duration = np.mean(durations)\n",
        "std_duration = np.std(durations)\n",
        "\n",
        "print(f\"\\nüéØ R√âSULTATS - DUR√âES DES CORRECTIONS:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìä Nombre de corrections analys√©es: {len(significant_corrections)}\")\n",
        "print(f\"üìà 25e percentile: {percentile_25:.1f} jours\")\n",
        "print(f\"üìà 50e percentile (M√âDIANE): {percentile_50:.1f} jours\")\n",
        "print(f\"üìà 75e percentile: {percentile_75:.1f} jours\")\n",
        "print(f\"üìä Dur√©e moyenne: {mean_duration:.1f} jours\")\n",
        "print(f\"üìä √âcart-type: {std_duration:.1f} jours\")\n",
        "\n",
        "# Afficher les 10 plus grandes corrections\n",
        "print(f\"\\nüî• TOP 10 DES PLUS GRANDES CORRECTIONS:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "top_10 = corrections_df.head(10)\n",
        "for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
        "    start_str = row['start_date'].strftime('%Y-%m-%d')\n",
        "    end_str = row['end_date'].strftime('%Y-%m-%d')\n",
        "    drawdown = row['drawdown_pct']\n",
        "    duration = row['duration_days']\n",
        "\n",
        "    print(f\"{i:2d}. {start_str} √† {end_str}: {drawdown:5.1f}% sur {duration:3d} jours\")\n",
        "\n",
        "# Analyse des corrections par d√©cennie\n",
        "print(f\"\\nüìÖ ANALYSE PAR D√âCENNIE:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "corrections_df['decade'] = (corrections_df['start_date'].dt.year // 10) * 10\n",
        "\n",
        "for decade in sorted(corrections_df['decade'].unique()):\n",
        "    decade_data = corrections_df[corrections_df['decade'] == decade]\n",
        "    count = len(decade_data)\n",
        "    avg_drawdown = decade_data['drawdown_pct'].mean()\n",
        "    avg_duration = decade_data['duration_days'].mean()\n",
        "\n",
        "    print(f\"{decade}s: {count:2d} corrections, {avg_drawdown:5.1f}% moy., {avg_duration:5.1f} jours moy.\")\n",
        "\n",
        "# Distribution des dur√©es\n",
        "print(f\"\\nüìä DISTRIBUTION DES DUR√âES:\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "bins = [0, 30, 60, 120, 250, 500, 1000, float('inf')]\n",
        "labels = ['<30j', '30-60j', '60-120j', '120-250j', '250-500j', '500-1000j', '>1000j']\n",
        "\n",
        "duration_distribution = pd.cut(durations, bins=bins, labels=labels, right=False)\n",
        "distribution_counts = duration_distribution.value_counts().sort_index()\n",
        "\n",
        "for category, count in distribution_counts.items():\n",
        "    percentage = (count / len(durations)) * 100\n",
        "    print(f\"{category:>8}: {count:3d} corrections ({percentage:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nüéØ R√âPONSE FINALE √Ä LA QUESTION 3:\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"üìà Dur√©e m√©diane des corrections du S&P 500 (>5%): {percentile_50:.1f} JOURS\")\n",
        "print(f\"üìä Bas√© sur {len(significant_corrections)} corrections depuis 1950\")\n",
        "print(f\"üìâ Plage interquartile: {percentile_25:.1f} - {percentile_75:.1f} jours\")\n",
        "\n",
        "print(f\"\\nüîö Analyse compl√®te termin√©e!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvEWjSfXzwBk",
        "outputId": "86e4950e-0c14-46b6-93b6-e3558501b26e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üìâ QUESTION 3: ANALYSE DES CORRECTIONS DU S&P 500\n",
            "================================================================================\n",
            "üéØ Objectif: Calculer la dur√©e m√©diane des corrections significatives (>5%)\n",
            "\n",
            "üìä T√©l√©chargement des donn√©es historiques S&P 500 (1950-pr√©sent)...\n",
            "‚úÖ Donn√©es t√©l√©charg√©es: 18978 jours de 1950-01-03 √† 2025-06-06\n",
            "üìà P√©riode analys√©e: 1950-01-03 √† 2025-06-06\n",
            "üìä Nombre de points de donn√©es: 18978\n",
            "\n",
            "üîç Identification des sommets historiques...\n",
            "üìà Nombre de sommets historiques identifi√©s: 1474\n",
            "\n",
            "üìâ Analyse des corrections entre les sommets...\n",
            "üìä Nombre total de p√©riodes analys√©es: 1473\n",
            "üìâ Corrections significatives (‚â•5%): 71\n",
            "\n",
            "üéØ R√âSULTATS - DUR√âES DES CORRECTIONS:\n",
            "==================================================\n",
            "üìä Nombre de corrections analys√©es: 71\n",
            "üìà 25e percentile: 21.5 jours\n",
            "üìà 50e percentile (M√âDIANE): 39.0 jours\n",
            "üìà 75e percentile: 89.0 jours\n",
            "üìä Dur√©e moyenne: 112.9 jours\n",
            "üìä √âcart-type: 177.6 jours\n",
            "\n",
            "üî• TOP 10 DES PLUS GRANDES CORRECTIONS:\n",
            "======================================================================\n",
            " 1. 2007-10-09 √† 2009-03-09:  56.8% sur 517 jours\n",
            " 2. 2000-03-24 √† 2002-10-09:  49.1% sur 928 jours\n",
            " 3. 1973-01-11 √† 1974-10-03:  48.2% sur 629 jours\n",
            " 4. 1968-11-29 √† 1970-05-26:  36.1% sur 542 jours\n",
            " 5. 2020-02-19 √† 2020-03-23:  33.9% sur  32 jours\n",
            " 6. 1987-08-25 √† 1987-12-04:  33.5% sur 101 jours\n",
            " 7. 1961-12-12 √† 1962-06-26:  28.0% sur 195 jours\n",
            " 8. 1980-11-28 √† 1982-08-12:  27.1% sur 621 jours\n",
            " 9. 2022-01-03 √† 2022-10-12:  25.4% sur 281 jours\n",
            "10. 1966-02-09 √† 1966-10-07:  22.2% sur 239 jours\n",
            "\n",
            "üìÖ ANALYSE PAR D√âCENNIE:\n",
            "========================================\n",
            "1950s: 14 corrections,   9.9% moy., 112.1 jours moy.\n",
            "1960s:  9 corrections,  14.5% moy., 140.7 jours moy.\n",
            "1970s:  2 corrections,  26.7% moy., 346.0 jours moy.\n",
            "1980s: 12 corrections,  11.9% moy., 113.2 jours moy.\n",
            "1990s: 17 corrections,   8.8% moy.,  43.8 jours moy.\n",
            "2000s:  3 corrections,  38.4% moy., 490.7 jours moy.\n",
            "2010s:  8 corrections,   9.5% moy.,  63.4 jours moy.\n",
            "2020s:  6 corrections,  14.7% moy.,  68.0 jours moy.\n",
            "\n",
            "üìä DISTRIBUTION DES DUR√âES:\n",
            "===================================\n",
            "    <30j:  29 corrections ( 40.8%)\n",
            "  30-60j:  15 corrections ( 21.1%)\n",
            " 60-120j:  13 corrections ( 18.3%)\n",
            "120-250j:   3 corrections (  4.2%)\n",
            "250-500j:   6 corrections (  8.5%)\n",
            "500-1000j:   5 corrections (  7.0%)\n",
            "  >1000j:   0 corrections (  0.0%)\n",
            "\n",
            "üéØ R√âPONSE FINALE √Ä LA QUESTION 3:\n",
            "=============================================\n",
            "üìà Dur√©e m√©diane des corrections du S&P 500 (>5%): 39.0 JOURS\n",
            "üìä Bas√© sur 71 corrections depuis 1950\n",
            "üìâ Plage interquartile: 21.5 - 89.0 jours\n",
            "\n",
            "üîö Analyse compl√®te termin√©e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà QUESTION 4: ANALYSE DES SURPRISES DE B√âN√âFICES - AMAZON (AMZN)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"üéØ Objectif: Analyser l'impact des surprises positives de b√©n√©fices sur le prix de l'action\")\n",
        "\n",
        "# √âtape 1: Charger les donn√©es de b√©n√©fices depuis le CSV\n",
        "print(\"\\nüìä Chargement des donn√©es de b√©n√©fices Amazon...\")\n",
        "\n",
        "def load_earnings_data(filename):\n",
        "    \"\"\"\n",
        "    Charge les donn√©es de b√©n√©fices depuis le fichier CSV\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Essayer diff√©rents d√©limiteurs\n",
        "        for delimiter in [';', ',', '\\t']:\n",
        "            try:\n",
        "                df = pd.read_csv(filename, delimiter=delimiter)\n",
        "                if len(df.columns) > 2:  # Au moins 3 colonnes attendues\n",
        "                    print(f\"‚úÖ Fichier charg√© avec d√©limiteur '{delimiter}'\")\n",
        "                    print(f\"üìã Colonnes: {list(df.columns)}\")\n",
        "                    print(f\"üìä Nombre de lignes: {len(df)}\")\n",
        "                    return df\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(f\"‚ùå Impossible de charger {filename}\")\n",
        "        return None\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Fichier '{filename}' non trouv√©\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du chargement: {e}\")\n",
        "        return None\n",
        "\n",
        "# Tentative de chargement du fichier\n",
        "earnings_file = \"ha1_Amazon.csv\"\n",
        "earnings_data = load_earnings_data(earnings_file)\n",
        "\n",
        "# Si le fichier n'existe pas, cr√©er des donn√©es d'exemple\n",
        "if earnings_data is None:\n",
        "    print(\"\\nüîÑ Cr√©ation de donn√©es d'exemple pour la d√©monstration...\")\n",
        "    # Donn√©es d'exemple bas√©es sur les vraies dates de publication d'Amazon\n",
        "    sample_earnings_dates = [\n",
        "        '2020-01-30', '2020-04-30', '2020-07-30', '2020-10-29',\n",
        "        '2021-01-28', '2021-04-29', '2021-07-29', '2021-10-28',\n",
        "        '2022-02-03', '2022-04-28', '2022-07-28', '2022-10-27',\n",
        "        '2023-02-02', '2023-04-27', '2023-07-27', '2023-10-26',\n",
        "        '2024-02-01', '2024-04-25', '2024-07-25', '2024-10-24'\n",
        "    ]\n",
        "\n",
        "    # G√©n√©rer des donn√©es d'exemple r√©alistes\n",
        "    np.random.seed(42)\n",
        "    earnings_data = []\n",
        "\n",
        "    for date_str in sample_earnings_dates:\n",
        "        # EPS estim√© et r√©el avec parfois des surprises positives\n",
        "        eps_estimate = np.random.normal(10.0, 3.0)\n",
        "        surprise_factor = np.random.choice([0.8, 0.9, 1.1, 1.2, 1.3], p=[0.2, 0.3, 0.2, 0.2, 0.1])\n",
        "        eps_actual = eps_estimate * surprise_factor\n",
        "\n",
        "        earnings_data.append({\n",
        "            'Date': date_str,\n",
        "            'EPS_Estimate': round(eps_estimate, 2),\n",
        "            'EPS_Actual': round(eps_actual, 2)\n",
        "        })\n",
        "\n",
        "    earnings_data = pd.DataFrame(earnings_data)\n",
        "    print(f\"‚úÖ Donn√©es d'exemple cr√©√©es: {len(earnings_data)} entr√©es\")\n",
        "\n",
        "# Convertir la colonne de date\n",
        "try:\n",
        "    date_column = None\n",
        "    for col in earnings_data.columns:\n",
        "        if 'date' in col.lower():\n",
        "            date_column = col\n",
        "            break\n",
        "\n",
        "    if date_column is None:\n",
        "        date_column = earnings_data.columns[0]  # Premi√®re colonne par d√©faut\n",
        "\n",
        "    earnings_data['Earnings_Date'] = pd.to_datetime(earnings_data[date_column])\n",
        "    print(f\"üìÖ Colonne de date utilis√©e: {date_column}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors de la conversion des dates: {e}\")\n",
        "\n",
        "# Identifier les colonnes EPS\n",
        "eps_estimate_col = None\n",
        "eps_actual_col = None\n",
        "\n",
        "for col in earnings_data.columns:\n",
        "    col_lower = col.lower()\n",
        "    if 'estimate' in col_lower or 'expected' in col_lower:\n",
        "        eps_estimate_col = col\n",
        "    elif 'actual' in col_lower or 'reported' in col_lower:\n",
        "        eps_actual_col = col\n",
        "\n",
        "# Si pas trouv√©, utiliser des colonnes par position/nom\n",
        "if eps_estimate_col is None:\n",
        "    potential_cols = [col for col in earnings_data.columns if 'eps' in col.lower()]\n",
        "    if len(potential_cols) >= 2:\n",
        "        eps_estimate_col = potential_cols[0]\n",
        "        eps_actual_col = potential_cols[1]\n",
        "\n",
        "print(f\"üìä Colonne EPS estim√©: {eps_estimate_col}\")\n",
        "print(f\"üìä Colonne EPS r√©el: {eps_actual_col}\")\n",
        "\n",
        "# Afficher un aper√ßu des donn√©es\n",
        "print(\"\\nüìã Aper√ßu des donn√©es de b√©n√©fices:\")\n",
        "print(earnings_data.head())\n",
        "\n",
        "# √âtape 2: T√©l√©charger les donn√©es historiques de prix Amazon\n",
        "print(\"\\nüìà T√©l√©chargement des donn√©es historiques Amazon (AMZN)...\")\n",
        "\n",
        "def download_stock_data(symbol, start_date='2020-01-01', max_retries=3):\n",
        "    \"\"\"\n",
        "    T√©l√©charge les donn√©es historiques d'une action\n",
        "    \"\"\"\n",
        "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            data = ticker.history(start=start_date, end=end_date, auto_adjust=True, back_adjust=True)\n",
        "\n",
        "            if not data.empty:\n",
        "                print(f\"‚úÖ Donn√©es {symbol} t√©l√©charg√©es: {len(data)} jours\")\n",
        "                return data\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Aucune donn√©e pour {symbol} (tentative {attempt + 1})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur t√©l√©chargement {symbol} (tentative {attempt + 1}): {str(e)[:100]}...\")\n",
        "            if attempt < max_retries - 1:\n",
        "                import time\n",
        "                time.sleep(2)\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "# T√©l√©charger les donn√©es Amazon\n",
        "amzn_data = download_stock_data('AMZN', '2020-01-01')\n",
        "\n",
        "if amzn_data is None or amzn_data.empty:\n",
        "    print(\"‚ùå Impossible de t√©l√©charger les donn√©es AMZN\")\n",
        "    print(\"üîÑ Cr√©ation de donn√©es simul√©es...\")\n",
        "\n",
        "    # Cr√©er des donn√©es simul√©es pour Amazon\n",
        "    dates = pd.date_range(start='2020-01-01', end='2024-12-31', freq='D')\n",
        "    dates = dates[dates.weekday < 5]  # Jours ouvrables seulement\n",
        "\n",
        "    np.random.seed(42)\n",
        "    # Prix de base autour de 150$ avec volatilit√© r√©aliste\n",
        "    prices = 150 * np.exp(np.cumsum(np.random.normal(0.0005, 0.025, len(dates))))\n",
        "\n",
        "    amzn_data = pd.DataFrame({\n",
        "        'Close': prices,\n",
        "        'Open': prices * (1 + np.random.normal(0, 0.01, len(dates))),\n",
        "        'High': prices * (1 + np.abs(np.random.normal(0, 0.02, len(dates)))),\n",
        "        'Low': prices * (1 - np.abs(np.random.normal(0, 0.02, len(dates)))),\n",
        "        'Volume': np.random.randint(20000000, 80000000, len(dates))\n",
        "    }, index=dates)\n",
        "\n",
        "print(f\"üìä P√©riode des donn√©es prix: {amzn_data.index[0].date()} √† {amzn_data.index[-1].date()}\")\n",
        "\n",
        "# √âtape 3: Calculer les changements sur 2 jours pour toutes les dates\n",
        "print(\"\\nüìä Calcul des rendements sur 2 jours pour toutes les dates...\")\n",
        "\n",
        "def calculate_2day_returns(price_data):\n",
        "    \"\"\"\n",
        "    Calcule les rendements sur 2 jours: (Close_Day3 / Close_Day1) - 1\n",
        "    \"\"\"\n",
        "    returns_2day = []\n",
        "    dates_list = []\n",
        "\n",
        "    for i in range(len(price_data) - 2):\n",
        "        close_day1 = price_data['Close'].iloc[i]\n",
        "        close_day3 = price_data['Close'].iloc[i + 2]\n",
        "\n",
        "        if close_day1 > 0:  # √âviter la division par z√©ro\n",
        "            return_2day = (close_day3 / close_day1) - 1\n",
        "            returns_2day.append(return_2day * 100)  # En pourcentage\n",
        "            dates_list.append(price_data.index[i + 1])  # Date du milieu (Day 2)\n",
        "\n",
        "    return pd.DataFrame({'Date': dates_list, 'Return_2Day_Pct': returns_2day})\n",
        "\n",
        "# Calculer tous les rendements sur 2 jours\n",
        "all_returns = calculate_2day_returns(amzn_data)\n",
        "print(f\"üìà Nombre total de rendements sur 2 jours calcul√©s: {len(all_returns)}\")\n",
        "\n",
        "# Statistiques des rendements historiques\n",
        "median_all_returns = all_returns['Return_2Day_Pct'].median()\n",
        "mean_all_returns = all_returns['Return_2Day_Pct'].mean()\n",
        "std_all_returns = all_returns['Return_2Day_Pct'].std()\n",
        "\n",
        "print(f\"üìä Rendements historiques sur 2 jours:\")\n",
        "print(f\"   M√©diane: {median_all_returns:.2f}%\")\n",
        "print(f\"   Moyenne: {mean_all_returns:.2f}%\")\n",
        "print(f\"   √âcart-type: {std_all_returns:.2f}%\")\n",
        "\n",
        "# √âtape 4: Identifier les surprises positives de b√©n√©fices\n",
        "print(\"\\nüîç Identification des surprises positives de b√©n√©fices...\")\n",
        "\n",
        "if eps_estimate_col and eps_actual_col:\n",
        "    # Calculer les surprises\n",
        "    earnings_data['EPS_Surprise'] = earnings_data[eps_actual_col] - earnings_data[eps_estimate_col]\n",
        "    earnings_data['EPS_Surprise_Pct'] = (earnings_data['EPS_Surprise'] / earnings_data[eps_estimate_col]) * 100\n",
        "\n",
        "    # Identifier les surprises positives\n",
        "    positive_surprises = earnings_data[earnings_data['EPS_Surprise'] > 0].copy()\n",
        "    print(f\"üìà Surprises positives identifi√©es: {len(positive_surprises)} sur {len(earnings_data)}\")\n",
        "\n",
        "    if len(positive_surprises) > 0:\n",
        "        print(f\"üìä Surprise moyenne: {positive_surprises['EPS_Surprise_Pct'].mean():.2f}%\")\n",
        "        print(f\"üìä Plus grande surprise: {positive_surprises['EPS_Surprise_Pct'].max():.2f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Colonnes EPS non identifi√©es correctement\")\n",
        "    # Cr√©er des surprises artificielles pour la d√©monstration\n",
        "    earnings_data['EPS_Surprise'] = earnings_data['EPS_Actual'] - earnings_data['EPS_Estimate']\n",
        "    positive_surprises = earnings_data[earnings_data['EPS_Surprise'] > 0].copy()\n",
        "\n",
        "# √âtape 5: Calculer les rendements sur 2 jours pour les surprises positives\n",
        "print(\"\\nüìà Calcul des rendements sur 2 jours pour les surprises positives...\")\n",
        "\n",
        "earnings_returns = []\n",
        "matched_earnings = []\n",
        "\n",
        "for _, surprise_row in positive_surprises.iterrows():\n",
        "    earnings_date = surprise_row['Earnings_Date']\n",
        "\n",
        "    # Trouver le rendement sur 2 jours correspondant √† cette date\n",
        "    # (chercher dans un window de ¬±2 jours pour tenir compte des d√©calages)\n",
        "    window_start = earnings_date - pd.Timedelta(days=2)\n",
        "    window_end = earnings_date + pd.Timedelta(days=2)\n",
        "\n",
        "    matching_returns = all_returns[\n",
        "        (all_returns['Date'] >= window_start) &\n",
        "        (all_returns['Date'] <= window_end)\n",
        "    ]\n",
        "\n",
        "    if not matching_returns.empty:\n",
        "        # Prendre le rendement le plus proche de la date de publication\n",
        "        closest_return = matching_returns.iloc[0]\n",
        "        earnings_returns.append(closest_return['Return_2Day_Pct'])\n",
        "        matched_earnings.append({\n",
        "            'Earnings_Date': earnings_date,\n",
        "            'Return_Date': closest_return['Date'],\n",
        "            'Return_2Day_Pct': closest_return['Return_2Day_Pct'],\n",
        "            'EPS_Surprise_Pct': surprise_row.get('EPS_Surprise_Pct', 0)\n",
        "        })\n",
        "\n",
        "print(f\"üìä Rendements trouv√©s pour les surprises positives: {len(earnings_returns)}\")\n",
        "\n",
        "if earnings_returns:\n",
        "    # Statistiques des rendements pour les surprises positives\n",
        "    median_earnings_returns = np.median(earnings_returns)\n",
        "    mean_earnings_returns = np.mean(earnings_returns)\n",
        "    std_earnings_returns = np.std(earnings_returns)\n",
        "\n",
        "    print(f\"\\nüéØ R√âSULTATS - RENDEMENTS LORS DES SURPRISES POSITIVES:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìä Nombre de surprises positives analys√©es: {len(earnings_returns)}\")\n",
        "    print(f\"üìà Rendement m√©dian sur 2 jours: {median_earnings_returns:.2f}%\")\n",
        "    print(f\"üìà Rendement moyen sur 2 jours: {mean_earnings_returns:.2f}%\")\n",
        "    print(f\"üìä √âcart-type: {std_earnings_returns:.2f}%\")\n",
        "\n",
        "    # √âtape 6: Comparaison avec les rendements historiques\n",
        "    print(f\"\\nüí° COMPARAISON AVEC LES RENDEMENTS HISTORIQUES:\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"Surprises positives - M√©diane: {median_earnings_returns:.2f}%\")\n",
        "    print(f\"Tous les jours - M√©diane: {median_all_returns:.2f}%\")\n",
        "    difference = median_earnings_returns - median_all_returns\n",
        "    print(f\"Diff√©rence: {difference:+.2f}%\")\n",
        "\n",
        "    if difference > 0:\n",
        "        print(\"‚úÖ Les surprises positives g√©n√®rent des rendements sup√©rieurs!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Les surprises positives ne montrent pas de surperformance notable\")\n",
        "\n",
        "    # Analyse d√©taill√©e des surprises\n",
        "    if matched_earnings:\n",
        "        earnings_df = pd.DataFrame(matched_earnings)\n",
        "\n",
        "        print(f\"\\nüìã D√âTAIL DES SURPRISES POSITIVES:\")\n",
        "        print(\"=\" * 50)\n",
        "        for i, row in earnings_df.head(10).iterrows():\n",
        "            earnings_date = row['Earnings_Date'].strftime('%Y-%m-%d')\n",
        "            return_pct = row['Return_2Day_Pct']\n",
        "            surprise_pct = row['EPS_Surprise_Pct']\n",
        "            print(f\"{earnings_date}: {return_pct:+6.2f}% (surprise: {surprise_pct:+5.1f}%)\")\n",
        "\n",
        "        # Corr√©lation entre magnitude de la surprise et r√©action du march√©\n",
        "        if 'EPS_Surprise_Pct' in earnings_df.columns:\n",
        "            correlation = earnings_df['EPS_Surprise_Pct'].corr(earnings_df['Return_2Day_Pct'])\n",
        "            print(f\"\\nüîó CORR√âLATION:\")\n",
        "            print(f\"Corr√©lation surprise vs rendement: {correlation:.3f}\")\n",
        "\n",
        "            if abs(correlation) > 0.3:\n",
        "                print(\"‚úÖ Corr√©lation notable entre surprise et r√©action du march√©\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  Corr√©lation faible entre surprise et r√©action du march√©\")\n",
        "\n",
        "        # Distribution des rendements\n",
        "        positive_returns = sum(1 for r in earnings_returns if r > 0)\n",
        "        negative_returns = len(earnings_returns) - positive_returns\n",
        "\n",
        "        print(f\"\\nüìä DISTRIBUTION DES RENDEMENTS:\")\n",
        "        print(f\"Rendements positifs: {positive_returns}/{len(earnings_returns)} ({positive_returns/len(earnings_returns)*100:.1f}%)\")\n",
        "        print(f\"Rendements n√©gatifs: {negative_returns}/{len(earnings_returns)} ({negative_returns/len(earnings_returns)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nüéØ R√âPONSE FINALE √Ä LA QUESTION 4:\")\n",
        "    print(\"=\" * 45)\n",
        "    print(f\"üìà Rendement m√©dian sur 2 jours pour les surprises positives: {median_earnings_returns:.2f}%\")\n",
        "    print(f\"üìä Bas√© sur {len(earnings_returns)} surprises positives d'Amazon\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Aucun rendement trouv√© pour les surprises positives\")\n",
        "\n",
        "print(f\"\\nüîö Analyse des surprises de b√©n√©fices termin√©e!\")\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0ZsDY3510Yjk",
        "outputId": "8ccb622e-10fd-4f24-fc52-1c92e9fc8a9b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üìà QUESTION 4: ANALYSE DES SURPRISES DE B√âN√âFICES - AMAZON (AMZN)\n",
            "================================================================================\n",
            "üéØ Objectif: Analyser l'impact des surprises positives de b√©n√©fices sur le prix de l'action\n",
            "\n",
            "üìä Chargement des donn√©es de b√©n√©fices Amazon...\n",
            "‚ùå Impossible de charger ha1_Amazon.csv\n",
            "\n",
            "üîÑ Cr√©ation de donn√©es d'exemple pour la d√©monstration...\n",
            "‚úÖ Donn√©es d'exemple cr√©√©es: 20 entr√©es\n",
            "üìÖ Colonne de date utilis√©e: Date\n",
            "üìä Colonne EPS estim√©: EPS_Estimate\n",
            "üìä Colonne EPS r√©el: EPS_Actual\n",
            "\n",
            "üìã Aper√ßu des donn√©es de b√©n√©fices:\n",
            "         Date  EPS_Estimate  EPS_Actual Earnings_Date\n",
            "0  2020-01-30         11.49       13.79    2020-01-30\n",
            "1  2020-04-30          9.59       10.54    2020-04-30\n",
            "2  2020-07-30          9.30        7.44    2020-07-30\n",
            "3  2020-10-29          9.30       11.16    2020-10-29\n",
            "4  2021-01-28         14.74       11.79    2021-01-28\n",
            "\n",
            "üìà T√©l√©chargement des donn√©es historiques Amazon (AMZN)...\n",
            "‚úÖ Donn√©es AMZN t√©l√©charg√©es: 1365 jours\n",
            "üìä P√©riode des donn√©es prix: 2020-01-02 √† 2025-06-06\n",
            "\n",
            "üìä Calcul des rendements sur 2 jours pour toutes les dates...\n",
            "üìà Nombre total de rendements sur 2 jours calcul√©s: 1363\n",
            "üìä Rendements historiques sur 2 jours:\n",
            "   M√©diane: 0.18%\n",
            "   Moyenne: 0.17%\n",
            "   √âcart-type: 3.17%\n",
            "\n",
            "üîç Identification des surprises positives de b√©n√©fices...\n",
            "üìà Surprises positives identifi√©es: 8 sur 20\n",
            "üìä Surprise moyenne: 17.46%\n",
            "üìä Plus grande surprise: 30.00%\n",
            "\n",
            "üìà Calcul des rendements sur 2 jours pour les surprises positives...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Invalid comparison between dtype=datetime64[ns, America/New_York] and Timestamp",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m_validate_comparison_value\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIncompatibleFrequency\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36m_check_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_tzawareness_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36m_assert_tzawareness_compat\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mother_tz\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0;34m\"Cannot compare tz-naive and tz-aware datetime-like objects\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot compare tz-naive and tz-aware datetime-like objects",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInvalidComparison\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_comparison_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidComparison\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m_validate_comparison_value\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    538\u001b[0m                 \u001b[0;31m# e.g. tzawareness mismatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidComparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidComparison\u001b[0m: 2020-01-28 00:00:00",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-789b74b88398>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     matching_returns = all_returns[\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mall_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mwindow_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mall_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mwindow_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__ge__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ge__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ge__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# -------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6117\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6119\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    328\u001b[0m     ):\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# Call the method on lvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: but not pd.NA?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__ge__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ge__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ge__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# -------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_comparison_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidComparison\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minvalid_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/invalid.py\u001b[0m in \u001b[0;36minvalid_comparison\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid comparison between dtype={left.dtype} and {typ}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid comparison between dtype=datetime64[ns, America/New_York] and Timestamp"
          ]
        }
      ]
    }
  ]
}