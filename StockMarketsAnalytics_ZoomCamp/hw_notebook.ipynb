{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EJE7jpA7ymzl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "ec_5htssy8xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TÃ©lÃ©charger les donnÃ©es S&P 500 depuis Wikipedia\n",
        "print(\"ğŸ“Š TÃ©lÃ©chargement des donnÃ©es S&P 500 depuis Wikipedia...\")\n",
        "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "\n",
        "tables = pd.read_html(url)\n",
        "sp500_df = tables[0]\n",
        "\n",
        "print(f\"âœ… DonnÃ©es tÃ©lÃ©chargÃ©es avec succÃ¨s!\")\n",
        "print(f\"ğŸ“ˆ Nombre d'entreprises: {len(sp500_df)}\")\n",
        "print(f\"ğŸ“‹ Colonnes disponibles: {list(sp500_df.columns)}\")\n",
        "\n",
        "# Identifier la colonne contenant les dates d'ajout\n",
        "date_column = None\n",
        "for col in sp500_df.columns:\n",
        "    if 'date' in col.lower() or 'added' in col.lower():\n",
        "        date_column = col\n",
        "        break\n",
        "\n",
        "# Fonction pour extraire l'annÃ©e\n",
        "def extract_year(date_str):\n",
        "    if pd.isna(date_str) or date_str == '':\n",
        "        return None\n",
        "\n",
        "    date_str = str(date_str).strip()\n",
        "\n",
        "    try:\n",
        "        # Format YYYY-MM-DD\n",
        "        if '-' in date_str and len(date_str.split('-')[0]) == 4:\n",
        "            return int(date_str.split('-')[0])\n",
        "        # Format MM/DD/YYYY\n",
        "        elif '/' in date_str:\n",
        "            parts = date_str.split('/')\n",
        "            if len(parts) == 3 and len(parts[2]) == 4:\n",
        "                return int(parts[2])\n",
        "        # Format YYYY seulement\n",
        "        elif date_str.isdigit() and len(date_str) == 4:\n",
        "            return int(date_str)\n",
        "        # Autres formats avec pandas\n",
        "        else:\n",
        "            parsed_date = pd.to_datetime(date_str, errors='coerce')\n",
        "            if not pd.isna(parsed_date):\n",
        "                return parsed_date.year\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return None\n",
        "\n",
        "# Extraire les annÃ©es d'ajout\n",
        "sp500_df['Year_Added'] = sp500_df[date_column].apply(extract_year)\n",
        "\n",
        "# Filtrer les donnÃ©es valides (exclure 1957 et les valeurs nulles)\n",
        "valid_years = sp500_df.dropna(subset=['Year_Added'])\n",
        "valid_years = valid_years[valid_years['Year_Added'] != 1957]\n",
        "\n",
        "print(f\"ğŸ“Š Entreprises avec annÃ©es d'ajout valides: {len(valid_years)}\")\n",
        "\n",
        "# Calculer le nombre d'ajouts par annÃ©e\n",
        "yearly_additions = valid_years['Year_Added'].value_counts().sort_index()\n",
        "\n",
        "# Trouver l'annÃ©e avec le plus d'ajouts\n",
        "max_additions_year = yearly_additions.idxmax()\n",
        "max_additions_count = yearly_additions.max()\n",
        "\n",
        "print(f\"\\nğŸ† RÃ‰PONSE Ã€ LA QUESTION:\")\n",
        "print(f\"L'annÃ©e avec le plus d'ajouts: {int(max_additions_year)}\")\n",
        "print(f\"Nombre d'ajouts cette annÃ©e-lÃ : {max_additions_count}\")\n",
        "\n",
        "# VÃ©rifier s'il y a plusieurs annÃ©es avec le mÃªme maximum\n",
        "years_with_max = yearly_additions[yearly_additions == max_additions_count]\n",
        "if len(years_with_max) > 1:\n",
        "    most_recent_max_year = years_with_max.index.max()\n",
        "    print(f\"Plusieurs annÃ©es ont {max_additions_count} ajouts.\")\n",
        "    print(f\"L'annÃ©e la plus rÃ©cente avec le maximum: {int(most_recent_max_year)}\")\n",
        "\n",
        "# Question additionnelle: Entreprises prÃ©sentes depuis plus de 20 ans\n",
        "current_year = 2025\n",
        "cutoff_year = current_year - 20\n",
        "companies_20_plus_years = valid_years[valid_years['Year_Added'] <= cutoff_year]\n",
        "\n",
        "print(f\"\\nğŸ“… QUESTION ADDITIONNELLE:\")\n",
        "print(f\"Entreprises prÃ©sentes depuis plus de 20 ans (ajoutÃ©es avant {cutoff_year}): {len(companies_20_plus_years)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHduy64Zy4mV",
        "outputId": "40cc1efe-441d-4776-e3bd-2fd7607e5f36"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š TÃ©lÃ©chargement des donnÃ©es S&P 500 depuis Wikipedia...\n",
            "âœ… DonnÃ©es tÃ©lÃ©chargÃ©es avec succÃ¨s!\n",
            "ğŸ“ˆ Nombre d'entreprises: 503\n",
            "ğŸ“‹ Colonnes disponibles: ['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry', 'Headquarters Location', 'Date added', 'CIK', 'Founded']\n",
            "ğŸ“Š Entreprises avec annÃ©es d'ajout valides: 450\n",
            "\n",
            "ğŸ† RÃ‰PONSE Ã€ LA QUESTION:\n",
            "L'annÃ©e avec le plus d'ajouts: 2016\n",
            "Nombre d'ajouts cette annÃ©e-lÃ : 23\n",
            "Plusieurs annÃ©es ont 23 ajouts.\n",
            "L'annÃ©e la plus rÃ©cente avec le maximum: 2017\n",
            "\n",
            "ğŸ“… QUESTION ADDITIONNELLE:\n",
            "Entreprises prÃ©sentes depuis plus de 20 ans (ajoutÃ©es avant 2005): 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2"
      ],
      "metadata": {
        "id": "H9T00jDVzBwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸŒ ANALYSE DES INDICES BOURSIERS MONDIAUX - YTD 2025\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# DÃ©finition des indices avec leurs symboles Yahoo Finance\n",
        "indices = {\n",
        "    'United States': '^GSPC',        # S&P 500\n",
        "    'China': '000001.SS',            # Shanghai Composite\n",
        "    'Hong Kong': '^HSI',             # Hang Seng Index\n",
        "    'Australia': '^AXJO',            # S&P/ASX 200\n",
        "    'India': '^NSEI',                # Nifty 50\n",
        "    'Canada': '^GSPTSE',             # S&P/TSX Composite\n",
        "    'Germany': '^GDAXI',             # DAX\n",
        "    'United Kingdom': '^FTSE',       # FTSE 100\n",
        "    'Japan': '^N225',                # Nikkei 225\n",
        "    'Mexico': '^MXX',                # IPC Mexico\n",
        "    'Brazil': '^BVSP'                # Ibovespa\n",
        "}\n",
        "\n",
        "# ParamÃ¨tres de date pour YTD 2025\n",
        "start_date = '2025-01-01'\n",
        "end_date = '2025-05-01'\n",
        "print(f\"ğŸ“… PÃ©riode d'analyse: {start_date} Ã  {end_date}\")\n",
        "print(f\"ğŸ“Š Nombre d'indices analysÃ©s: {len(indices)}\")\n",
        "\n",
        "# Fonction optimisÃ©e pour tÃ©lÃ©charger les donnÃ©es\n",
        "def get_ytd_performance(symbol, start_date, end_date):\n",
        "    \"\"\"TÃ©lÃ©charge les donnÃ©es et calcule la performance YTD\"\"\"\n",
        "    try:\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        data = ticker.history(start=start_date, end=end_date, auto_adjust=True)\n",
        "\n",
        "        if data.empty:\n",
        "            return None\n",
        "\n",
        "        first_price = data['Close'].iloc[0]\n",
        "        last_price = data['Close'].iloc[-1]\n",
        "        ytd_return = ((last_price - first_price) / first_price) * 100\n",
        "\n",
        "        return {\n",
        "            'Symbol': symbol,\n",
        "            'Start_Price': first_price,\n",
        "            'End_Price': last_price,\n",
        "            'YTD_Return_%': ytd_return\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur pour {symbol}: {str(e)[:50]}...\")\n",
        "        return None\n",
        "\n",
        "# TÃ©lÃ©chargement des donnÃ©es\n",
        "print(f\"\\nğŸ“ˆ TÃ©lÃ©chargement des donnÃ©es...\")\n",
        "detailed_data = {}\n",
        "\n",
        "for country, symbol in indices.items():\n",
        "    print(f\"   TÃ©lÃ©chargement: {country} ({symbol})\")\n",
        "    result = get_ytd_performance(symbol, start_date, end_date)\n",
        "\n",
        "    if result:\n",
        "        detailed_data[country] = result\n",
        "        print(f\"   âœ… {country}: {result['YTD_Return_%']:+.2f}%\")\n",
        "    else:\n",
        "        print(f\"   âŒ Ã‰chec pour {country}\")\n",
        "\n",
        "print(f\"\\nâœ… DonnÃ©es tÃ©lÃ©chargÃ©es avec succÃ¨s pour {len(detailed_data)}/{len(indices)} indices\")\n",
        "\n",
        "# CrÃ©ation du DataFrame et analyse\n",
        "if detailed_data:\n",
        "    df_results = pd.DataFrame.from_dict(detailed_data, orient='index')\n",
        "    df_results = df_results.sort_values('YTD_Return_%', ascending=False)\n",
        "\n",
        "    # Affichage des rÃ©sultats YTD\n",
        "    print(f\"\\nğŸ† PERFORMANCES YTD (1 Jan - 1 Mai 2025):\")\n",
        "    print(\"=\" * 60)\n",
        "    for i, (country, row) in enumerate(df_results.iterrows(), 1):\n",
        "        symbol = row['Symbol']\n",
        "        ytd_return = row['YTD_Return_%']\n",
        "        status = \"ğŸ“ˆ\" if ytd_return > 0 else \"ğŸ“‰\"\n",
        "        print(f\"{i:2d}. {country:<15} ({symbol:<10}): {ytd_return:+7.2f}% {status}\")\n",
        "\n",
        "    # RÃ©ponse Ã  la question principale\n",
        "    if 'United States' in detailed_data:\n",
        "        sp500_return = detailed_data['United States']['YTD_Return_%']\n",
        "        better_than_sp500 = sum(1 for country, data in detailed_data.items()\n",
        "                              if data['YTD_Return_%'] > sp500_return and country != 'United States')\n",
        "\n",
        "        print(f\"\\nğŸ¯ RÃ‰PONSE Ã€ LA QUESTION:\")\n",
        "        print(f\"S&P 500 (Ã‰tats-Unis) YTD: {sp500_return:+.2f}%\")\n",
        "        print(f\"Indices avec de meilleures performances: {better_than_sp500} sur {len(detailed_data)-1}\")\n",
        "\n",
        "    # Statistiques supplÃ©mentaires\n",
        "    returns = [data['YTD_Return_%'] for data in detailed_data.values()]\n",
        "    positive_returns = sum(1 for ret in returns if ret > 0)\n",
        "    negative_returns = len(returns) - positive_returns\n",
        "    avg_return = np.mean(returns)\n",
        "\n",
        "    print(f\"\\nğŸ“Š STATISTIQUES SUPPLÃ‰MENTAIRES:\")\n",
        "    print(f\"Rendements positifs: {positive_returns}/{len(returns)}\")\n",
        "    print(f\"Rendements nÃ©gatifs: {negative_returns}/{len(returns)}\")\n",
        "    print(f\"Rendement moyen: {avg_return:+.2f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nâŒ Aucune donnÃ©e n'a pu Ãªtre tÃ©lÃ©chargÃ©e. VÃ©rifiez votre connexion internet.\")\n",
        "\n",
        "print(f\"\\nğŸ”š Analyse terminÃ©e!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_8Zz2dvzGqs",
        "outputId": "3d18c10a-bbc5-4418-e6a6-1257ed970e85"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸŒ ANALYSE DES INDICES BOURSIERS MONDIAUX - YTD 2025\n",
            "================================================================================\n",
            "ğŸ“… PÃ©riode d'analyse: 2025-01-01 Ã  2025-05-01\n",
            "ğŸ“Š Nombre d'indices analysÃ©s: 11\n",
            "\n",
            "ğŸ“ˆ TÃ©lÃ©chargement des donnÃ©es...\n",
            "   TÃ©lÃ©chargement: United States (^GSPC)\n",
            "   âœ… United States: -5.10%\n",
            "   TÃ©lÃ©chargement: China (000001.SS)\n",
            "   âœ… China: +0.50%\n",
            "   TÃ©lÃ©chargement: Hong Kong (^HSI)\n",
            "   âœ… Hong Kong: +12.72%\n",
            "   TÃ©lÃ©chargement: Australia (^AXJO)\n",
            "   âœ… Australia: -0.91%\n",
            "   TÃ©lÃ©chargement: India (^NSEI)\n",
            "   âœ… India: +2.49%\n",
            "   TÃ©lÃ©chargement: Canada (^GSPTSE)\n",
            "   âœ… Canada: -0.23%\n",
            "   TÃ©lÃ©chargement: Germany (^GDAXI)\n",
            "   âœ… Germany: +12.35%\n",
            "   TÃ©lÃ©chargement: United Kingdom (^FTSE)\n",
            "   âœ… United Kingdom: +2.84%\n",
            "   TÃ©lÃ©chargement: Japan (^N225)\n",
            "   âœ… Japan: -8.30%\n",
            "   TÃ©lÃ©chargement: Mexico (^MXX)\n",
            "   âœ… Mexico: +13.05%\n",
            "   TÃ©lÃ©chargement: Brazil (^BVSP)\n",
            "   âœ… Brazil: +12.44%\n",
            "\n",
            "âœ… DonnÃ©es tÃ©lÃ©chargÃ©es avec succÃ¨s pour 11/11 indices\n",
            "\n",
            "ğŸ† PERFORMANCES YTD (1 Jan - 1 Mai 2025):\n",
            "============================================================\n",
            " 1. Mexico          (^MXX      ):  +13.05% ğŸ“ˆ\n",
            " 2. Hong Kong       (^HSI      ):  +12.72% ğŸ“ˆ\n",
            " 3. Brazil          (^BVSP     ):  +12.44% ğŸ“ˆ\n",
            " 4. Germany         (^GDAXI    ):  +12.35% ğŸ“ˆ\n",
            " 5. United Kingdom  (^FTSE     ):   +2.84% ğŸ“ˆ\n",
            " 6. India           (^NSEI     ):   +2.49% ğŸ“ˆ\n",
            " 7. China           (000001.SS ):   +0.50% ğŸ“ˆ\n",
            " 8. Canada          (^GSPTSE   ):   -0.23% ğŸ“‰\n",
            " 9. Australia       (^AXJO     ):   -0.91% ğŸ“‰\n",
            "10. United States   (^GSPC     ):   -5.10% ğŸ“‰\n",
            "11. Japan           (^N225     ):   -8.30% ğŸ“‰\n",
            "\n",
            "ğŸ¯ RÃ‰PONSE Ã€ LA QUESTION:\n",
            "S&P 500 (Ã‰tats-Unis) YTD: -5.10%\n",
            "Indices avec de meilleures performances: 9 sur 10\n",
            "\n",
            "ğŸ“Š STATISTIQUES SUPPLÃ‰MENTAIRES:\n",
            "Rendements positifs: 7/11\n",
            "Rendements nÃ©gatifs: 4/11\n",
            "Rendement moyen: +3.80%\n",
            "\n",
            "ğŸ”š Analyse terminÃ©e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“‰ QUESTION 3: ANALYSE DES CORRECTIONS DU S&P 500\")\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ¯ Objectif: Calculer la durÃ©e mÃ©diane des corrections significatives (>5%)\")\n",
        "\n",
        "# TÃ©lÃ©chargement des donnÃ©es historiques S&P 500 depuis 1950\n",
        "print(\"\\nğŸ“Š TÃ©lÃ©chargement des donnÃ©es historiques S&P 500 (1950-prÃ©sent)...\")\n",
        "start_date_historical = '1950-01-01'\n",
        "end_date_historical = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "# Fonction optimisÃ©e pour tÃ©lÃ©charger les donnÃ©es\n",
        "def download_sp500_historical(start_date, end_date):\n",
        "    \"\"\"TÃ©lÃ©charge les donnÃ©es historiques du S&P 500\"\"\"\n",
        "    try:\n",
        "        ticker = yf.Ticker('^GSPC')\n",
        "        data = ticker.history(start=start_date, end=end_date, auto_adjust=True)\n",
        "\n",
        "        if not data.empty:\n",
        "            print(f\"âœ… DonnÃ©es tÃ©lÃ©chargÃ©es: {len(data)} jours de {data.index[0].date()} Ã  {data.index[-1].date()}\")\n",
        "            return data\n",
        "        else:\n",
        "            print(\"âš ï¸ Aucune donnÃ©e obtenue\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur tÃ©lÃ©chargement: {str(e)[:100]}...\")\n",
        "        return None\n",
        "\n",
        "# TÃ©lÃ©chargement des donnÃ©es\n",
        "sp500_data = download_sp500_historical(start_date_historical, end_date_historical)\n",
        "\n",
        "if sp500_data is None or sp500_data.empty:\n",
        "    print(\"âŒ Impossible de tÃ©lÃ©charger les donnÃ©es S&P 500\")\n",
        "    exit()\n",
        "\n",
        "print(f\"ğŸ“ˆ PÃ©riode analysÃ©e: {sp500_data.index[0].date()} Ã  {sp500_data.index[-1].date()}\")\n",
        "print(f\"ğŸ“Š Nombre de points de donnÃ©es: {len(sp500_data)}\")\n",
        "\n",
        "# Identifier les sommets historiques (all-time highs)\n",
        "print(\"\\nğŸ” Identification des sommets historiques...\")\n",
        "sp500_data['Cumulative_Max'] = sp500_data['Close'].expanding().max()\n",
        "sp500_data['Is_ATH'] = sp500_data['Close'] == sp500_data['Cumulative_Max']\n",
        "\n",
        "ath_dates = sp500_data[sp500_data['Is_ATH']].index.tolist()\n",
        "print(f\"ğŸ“ˆ Nombre de sommets historiques identifiÃ©s: {len(ath_dates)}\")\n",
        "\n",
        "# Analyser les corrections entre les ATH consÃ©cutifs\n",
        "print(\"\\nğŸ“‰ Analyse des corrections entre les sommets...\")\n",
        "corrections = []\n",
        "\n",
        "for i in range(len(ath_dates) - 1):\n",
        "    start_date = ath_dates[i]\n",
        "    end_date = ath_dates[i + 1]\n",
        "\n",
        "    period_data = sp500_data.loc[start_date:end_date]\n",
        "\n",
        "    if len(period_data) < 2:\n",
        "        continue\n",
        "\n",
        "    high_price = period_data['Close'].iloc[0]\n",
        "    min_price = period_data['Close'].min()\n",
        "    min_date = period_data['Close'].idxmin()\n",
        "\n",
        "    drawdown_pct = ((high_price - min_price) / high_price) * 100\n",
        "    duration_days = (min_date - start_date).days\n",
        "\n",
        "    corrections.append({\n",
        "        'start_date': start_date,\n",
        "        'end_date': min_date,\n",
        "        'high_price': high_price,\n",
        "        'low_price': min_price,\n",
        "        'drawdown_pct': drawdown_pct,\n",
        "        'duration_days': duration_days\n",
        "    })\n",
        "\n",
        "print(f\"ğŸ“Š Nombre total de pÃ©riodes analysÃ©es: {len(corrections)}\")\n",
        "\n",
        "# Filtrer les corrections significatives (>5%)\n",
        "significant_corrections = [c for c in corrections if c['drawdown_pct'] >= 5.0]\n",
        "print(f\"ğŸ“‰ Corrections significatives (â‰¥5%): {len(significant_corrections)}\")\n",
        "\n",
        "if not significant_corrections:\n",
        "    print(\"âŒ Aucune correction significative trouvÃ©e dans les donnÃ©es\")\n",
        "    exit()\n",
        "\n",
        "# CrÃ©er un DataFrame pour l'analyse\n",
        "corrections_df = pd.DataFrame(significant_corrections)\n",
        "corrections_df = corrections_df.sort_values('drawdown_pct', ascending=False)\n",
        "\n",
        "# Calculer les statistiques des durÃ©es\n",
        "durations = [c['duration_days'] for c in significant_corrections]\n",
        "\n",
        "percentile_25 = np.percentile(durations, 25)\n",
        "percentile_50 = np.percentile(durations, 50)  # MÃ©diane\n",
        "percentile_75 = np.percentile(durations, 75)\n",
        "mean_duration = np.mean(durations)\n",
        "std_duration = np.std(durations)\n",
        "\n",
        "print(f\"\\nğŸ¯ RÃ‰SULTATS - DURÃ‰ES DES CORRECTIONS:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ğŸ“Š Nombre de corrections analysÃ©es: {len(significant_corrections)}\")\n",
        "print(f\"ğŸ“ˆ 25e percentile: {percentile_25:.1f} jours\")\n",
        "print(f\"ğŸ“ˆ 50e percentile (MÃ‰DIANE): {percentile_50:.1f} jours\")\n",
        "print(f\"ğŸ“ˆ 75e percentile: {percentile_75:.1f} jours\")\n",
        "print(f\"ğŸ“Š DurÃ©e moyenne: {mean_duration:.1f} jours\")\n",
        "print(f\"ğŸ“Š Ã‰cart-type: {std_duration:.1f} jours\")\n",
        "\n",
        "# Afficher les 10 plus grandes corrections\n",
        "print(f\"\\nğŸ”¥ TOP 10 DES PLUS GRANDES CORRECTIONS:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "top_10 = corrections_df.head(10)\n",
        "for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
        "    start_str = row['start_date'].strftime('%Y-%m-%d')\n",
        "    end_str = row['end_date'].strftime('%Y-%m-%d')\n",
        "    drawdown = row['drawdown_pct']\n",
        "    duration = row['duration_days']\n",
        "\n",
        "    print(f\"{i:2d}. {start_str} Ã  {end_str}: {drawdown:5.1f}% sur {duration:3d} jours\")\n",
        "\n",
        "# Analyse des corrections par dÃ©cennie\n",
        "print(f\"\\nğŸ“… ANALYSE PAR DÃ‰CENNIE:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "corrections_df['decade'] = (corrections_df['start_date'].dt.year // 10) * 10\n",
        "\n",
        "for decade in sorted(corrections_df['decade'].unique()):\n",
        "    decade_data = corrections_df[corrections_df['decade'] == decade]\n",
        "    count = len(decade_data)\n",
        "    avg_drawdown = decade_data['drawdown_pct'].mean()\n",
        "    avg_duration = decade_data['duration_days'].mean()\n",
        "\n",
        "    print(f\"{decade}s: {count:2d} corrections, {avg_drawdown:5.1f}% moy., {avg_duration:5.1f} jours moy.\")\n",
        "\n",
        "# Distribution des durÃ©es\n",
        "print(f\"\\nğŸ“Š DISTRIBUTION DES DURÃ‰ES:\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "bins = [0, 30, 60, 120, 250, 500, 1000, float('inf')]\n",
        "labels = ['<30j', '30-60j', '60-120j', '120-250j', '250-500j', '500-1000j', '>1000j']\n",
        "\n",
        "duration_distribution = pd.cut(durations, bins=bins, labels=labels, right=False)\n",
        "distribution_counts = duration_distribution.value_counts().sort_index()\n",
        "\n",
        "for category, count in distribution_counts.items():\n",
        "    percentage = (count / len(durations)) * 100\n",
        "    print(f\"{category:>8}: {count:3d} corrections ({percentage:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nğŸ¯ RÃ‰PONSE FINALE Ã€ LA QUESTION 3:\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"ğŸ“ˆ DurÃ©e mÃ©diane des corrections du S&P 500 (>5%): {percentile_50:.1f} JOURS\")\n",
        "print(f\"ğŸ“Š BasÃ© sur {len(significant_corrections)} corrections depuis 1950\")\n",
        "print(f\"ğŸ“‰ Plage interquartile: {percentile_25:.1f} - {percentile_75:.1f} jours\")\n",
        "\n",
        "print(f\"\\nğŸ”š Analyse complÃ¨te terminÃ©e!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvEWjSfXzwBk",
        "outputId": "86e4950e-0c14-46b6-93b6-e3558501b26e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ“‰ QUESTION 3: ANALYSE DES CORRECTIONS DU S&P 500\n",
            "================================================================================\n",
            "ğŸ¯ Objectif: Calculer la durÃ©e mÃ©diane des corrections significatives (>5%)\n",
            "\n",
            "ğŸ“Š TÃ©lÃ©chargement des donnÃ©es historiques S&P 500 (1950-prÃ©sent)...\n",
            "âœ… DonnÃ©es tÃ©lÃ©chargÃ©es: 18978 jours de 1950-01-03 Ã  2025-06-06\n",
            "ğŸ“ˆ PÃ©riode analysÃ©e: 1950-01-03 Ã  2025-06-06\n",
            "ğŸ“Š Nombre de points de donnÃ©es: 18978\n",
            "\n",
            "ğŸ” Identification des sommets historiques...\n",
            "ğŸ“ˆ Nombre de sommets historiques identifiÃ©s: 1474\n",
            "\n",
            "ğŸ“‰ Analyse des corrections entre les sommets...\n",
            "ğŸ“Š Nombre total de pÃ©riodes analysÃ©es: 1473\n",
            "ğŸ“‰ Corrections significatives (â‰¥5%): 71\n",
            "\n",
            "ğŸ¯ RÃ‰SULTATS - DURÃ‰ES DES CORRECTIONS:\n",
            "==================================================\n",
            "ğŸ“Š Nombre de corrections analysÃ©es: 71\n",
            "ğŸ“ˆ 25e percentile: 21.5 jours\n",
            "ğŸ“ˆ 50e percentile (MÃ‰DIANE): 39.0 jours\n",
            "ğŸ“ˆ 75e percentile: 89.0 jours\n",
            "ğŸ“Š DurÃ©e moyenne: 112.9 jours\n",
            "ğŸ“Š Ã‰cart-type: 177.6 jours\n",
            "\n",
            "ğŸ”¥ TOP 10 DES PLUS GRANDES CORRECTIONS:\n",
            "======================================================================\n",
            " 1. 2007-10-09 Ã  2009-03-09:  56.8% sur 517 jours\n",
            " 2. 2000-03-24 Ã  2002-10-09:  49.1% sur 928 jours\n",
            " 3. 1973-01-11 Ã  1974-10-03:  48.2% sur 629 jours\n",
            " 4. 1968-11-29 Ã  1970-05-26:  36.1% sur 542 jours\n",
            " 5. 2020-02-19 Ã  2020-03-23:  33.9% sur  32 jours\n",
            " 6. 1987-08-25 Ã  1987-12-04:  33.5% sur 101 jours\n",
            " 7. 1961-12-12 Ã  1962-06-26:  28.0% sur 195 jours\n",
            " 8. 1980-11-28 Ã  1982-08-12:  27.1% sur 621 jours\n",
            " 9. 2022-01-03 Ã  2022-10-12:  25.4% sur 281 jours\n",
            "10. 1966-02-09 Ã  1966-10-07:  22.2% sur 239 jours\n",
            "\n",
            "ğŸ“… ANALYSE PAR DÃ‰CENNIE:\n",
            "========================================\n",
            "1950s: 14 corrections,   9.9% moy., 112.1 jours moy.\n",
            "1960s:  9 corrections,  14.5% moy., 140.7 jours moy.\n",
            "1970s:  2 corrections,  26.7% moy., 346.0 jours moy.\n",
            "1980s: 12 corrections,  11.9% moy., 113.2 jours moy.\n",
            "1990s: 17 corrections,   8.8% moy.,  43.8 jours moy.\n",
            "2000s:  3 corrections,  38.4% moy., 490.7 jours moy.\n",
            "2010s:  8 corrections,   9.5% moy.,  63.4 jours moy.\n",
            "2020s:  6 corrections,  14.7% moy.,  68.0 jours moy.\n",
            "\n",
            "ğŸ“Š DISTRIBUTION DES DURÃ‰ES:\n",
            "===================================\n",
            "    <30j:  29 corrections ( 40.8%)\n",
            "  30-60j:  15 corrections ( 21.1%)\n",
            " 60-120j:  13 corrections ( 18.3%)\n",
            "120-250j:   3 corrections (  4.2%)\n",
            "250-500j:   6 corrections (  8.5%)\n",
            "500-1000j:   5 corrections (  7.0%)\n",
            "  >1000j:   0 corrections (  0.0%)\n",
            "\n",
            "ğŸ¯ RÃ‰PONSE FINALE Ã€ LA QUESTION 3:\n",
            "=============================================\n",
            "ğŸ“ˆ DurÃ©e mÃ©diane des corrections du S&P 500 (>5%): 39.0 JOURS\n",
            "ğŸ“Š BasÃ© sur 71 corrections depuis 1950\n",
            "ğŸ“‰ Plage interquartile: 21.5 - 89.0 jours\n",
            "\n",
            "ğŸ”š Analyse complÃ¨te terminÃ©e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“ˆ QUESTION 4: ANALYSE DES SURPRISES DE BÃ‰NÃ‰FICES - AMAZON (AMZN)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ¯ Objectif: Analyser l'impact des surprises positives de bÃ©nÃ©fices sur le prix de l'action\")\n",
        "\n",
        "# Ã‰tape 1: Charger les donnÃ©es de bÃ©nÃ©fices depuis le CSV\n",
        "print(\"\\nğŸ“Š Chargement des donnÃ©es de bÃ©nÃ©fices Amazon...\")\n",
        "\n",
        "def load_earnings_data(filename):\n",
        "    \"\"\"\n",
        "    Charge les donnÃ©es de bÃ©nÃ©fices depuis le fichier CSV\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Essayer diffÃ©rents dÃ©limiteurs\n",
        "        for delimiter in [';', ',', '\\t']:\n",
        "            try:\n",
        "                df = pd.read_csv(filename, delimiter=delimiter)\n",
        "                if len(df.columns) > 2:  # Au moins 3 colonnes attendues\n",
        "                    print(f\"âœ… Fichier chargÃ© avec dÃ©limiteur '{delimiter}'\")\n",
        "                    print(f\"ğŸ“‹ Colonnes: {list(df.columns)}\")\n",
        "                    print(f\"ğŸ“Š Nombre de lignes: {len(df)}\")\n",
        "                    return df\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(f\"âŒ Impossible de charger {filename}\")\n",
        "        return None\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Fichier '{filename}' non trouvÃ©\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur lors du chargement: {e}\")\n",
        "        return None\n",
        "\n",
        "# Tentative de chargement du fichier\n",
        "earnings_file = \"ha1_Amazon.csv\"\n",
        "earnings_data = load_earnings_data(earnings_file)\n",
        "\n",
        "# Si le fichier n'existe pas, crÃ©er des donnÃ©es d'exemple\n",
        "if earnings_data is None:\n",
        "    print(\"\\nğŸ”„ CrÃ©ation de donnÃ©es d'exemple pour la dÃ©monstration...\")\n",
        "    # DonnÃ©es d'exemple basÃ©es sur les vraies dates de publication d'Amazon\n",
        "    sample_earnings_dates = [\n",
        "        '2020-01-30', '2020-04-30', '2020-07-30', '2020-10-29',\n",
        "        '2021-01-28', '2021-04-29', '2021-07-29', '2021-10-28',\n",
        "        '2022-02-03', '2022-04-28', '2022-07-28', '2022-10-27',\n",
        "        '2023-02-02', '2023-04-27', '2023-07-27', '2023-10-26',\n",
        "        '2024-02-01', '2024-04-25', '2024-07-25', '2024-10-24'\n",
        "    ]\n",
        "\n",
        "    # GÃ©nÃ©rer des donnÃ©es d'exemple rÃ©alistes\n",
        "    np.random.seed(42)\n",
        "    earnings_data = []\n",
        "\n",
        "    for date_str in sample_earnings_dates:\n",
        "        # EPS estimÃ© et rÃ©el avec parfois des surprises positives\n",
        "        eps_estimate = np.random.normal(10.0, 3.0)\n",
        "        surprise_factor = np.random.choice([0.8, 0.9, 1.1, 1.2, 1.3], p=[0.2, 0.3, 0.2, 0.2, 0.1])\n",
        "        eps_actual = eps_estimate * surprise_factor\n",
        "\n",
        "        earnings_data.append({\n",
        "            'Date': date_str,\n",
        "            'EPS_Estimate': round(eps_estimate, 2),\n",
        "            'EPS_Actual': round(eps_actual, 2)\n",
        "        })\n",
        "\n",
        "    earnings_data = pd.DataFrame(earnings_data)\n",
        "    print(f\"âœ… DonnÃ©es d'exemple crÃ©Ã©es: {len(earnings_data)} entrÃ©es\")\n",
        "\n",
        "# Convertir la colonne de date\n",
        "try:\n",
        "    date_column = None\n",
        "    for col in earnings_data.columns:\n",
        "        if 'date' in col.lower():\n",
        "            date_column = col\n",
        "            break\n",
        "\n",
        "    if date_column is None:\n",
        "        date_column = earnings_data.columns[0]  # PremiÃ¨re colonne par dÃ©faut\n",
        "\n",
        "    earnings_data['Earnings_Date'] = pd.to_datetime(earnings_data[date_column])\n",
        "    print(f\"ğŸ“… Colonne de date utilisÃ©e: {date_column}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur lors de la conversion des dates: {e}\")\n",
        "\n",
        "# Identifier les colonnes EPS\n",
        "eps_estimate_col = None\n",
        "eps_actual_col = None\n",
        "\n",
        "for col in earnings_data.columns:\n",
        "    col_lower = col.lower()\n",
        "    if 'estimate' in col_lower or 'expected' in col_lower:\n",
        "        eps_estimate_col = col\n",
        "    elif 'actual' in col_lower or 'reported' in col_lower:\n",
        "        eps_actual_col = col\n",
        "\n",
        "# Si pas trouvÃ©, utiliser des colonnes par position/nom\n",
        "if eps_estimate_col is None:\n",
        "    potential_cols = [col for col in earnings_data.columns if 'eps' in col.lower()]\n",
        "    if len(potential_cols) >= 2:\n",
        "        eps_estimate_col = potential_cols[0]\n",
        "        eps_actual_col = potential_cols[1]\n",
        "\n",
        "print(f\"ğŸ“Š Colonne EPS estimÃ©: {eps_estimate_col}\")\n",
        "print(f\"ğŸ“Š Colonne EPS rÃ©el: {eps_actual_col}\")\n",
        "\n",
        "# Afficher un aperÃ§u des donnÃ©es\n",
        "print(\"\\nğŸ“‹ AperÃ§u des donnÃ©es de bÃ©nÃ©fices:\")\n",
        "print(earnings_data.head())\n",
        "\n",
        "# Ã‰tape 2: TÃ©lÃ©charger les donnÃ©es historiques de prix Amazon\n",
        "print(\"\\nğŸ“ˆ TÃ©lÃ©chargement des donnÃ©es historiques Amazon (AMZN)...\")\n",
        "\n",
        "def download_stock_data(symbol, start_date='2020-01-01', max_retries=3):\n",
        "    \"\"\"\n",
        "    TÃ©lÃ©charge les donnÃ©es historiques d'une action\n",
        "    \"\"\"\n",
        "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            data = ticker.history(start=start_date, end=end_date, auto_adjust=True, back_adjust=True)\n",
        "\n",
        "            if not data.empty:\n",
        "                print(f\"âœ… DonnÃ©es {symbol} tÃ©lÃ©chargÃ©es: {len(data)} jours\")\n",
        "                return data\n",
        "            else:\n",
        "                print(f\"âš ï¸  Aucune donnÃ©e pour {symbol} (tentative {attempt + 1})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur tÃ©lÃ©chargement {symbol} (tentative {attempt + 1}): {str(e)[:100]}...\")\n",
        "            if attempt < max_retries - 1:\n",
        "                import time\n",
        "                time.sleep(2)\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "# TÃ©lÃ©charger les donnÃ©es Amazon\n",
        "amzn_data = download_stock_data('AMZN', '2020-01-01')\n",
        "\n",
        "if amzn_data is None or amzn_data.empty:\n",
        "    print(\"âŒ Impossible de tÃ©lÃ©charger les donnÃ©es AMZN\")\n",
        "    print(\"ğŸ”„ CrÃ©ation de donnÃ©es simulÃ©es...\")\n",
        "\n",
        "    # CrÃ©er des donnÃ©es simulÃ©es pour Amazon\n",
        "    dates = pd.date_range(start='2020-01-01', end='2024-12-31', freq='D')\n",
        "    dates = dates[dates.weekday < 5]  # Jours ouvrables seulement\n",
        "\n",
        "    np.random.seed(42)\n",
        "    # Prix de base autour de 150$ avec volatilitÃ© rÃ©aliste\n",
        "    prices = 150 * np.exp(np.cumsum(np.random.normal(0.0005, 0.025, len(dates))))\n",
        "\n",
        "    amzn_data = pd.DataFrame({\n",
        "        'Close': prices,\n",
        "        'Open': prices * (1 + np.random.normal(0, 0.01, len(dates))),\n",
        "        'High': prices * (1 + np.abs(np.random.normal(0, 0.02, len(dates)))),\n",
        "        'Low': prices * (1 - np.abs(np.random.normal(0, 0.02, len(dates)))),\n",
        "        'Volume': np.random.randint(20000000, 80000000, len(dates))\n",
        "    }, index=dates)\n",
        "\n",
        "print(f\"ğŸ“Š PÃ©riode des donnÃ©es prix: {amzn_data.index[0].date()} Ã  {amzn_data.index[-1].date()}\")\n",
        "\n",
        "# Ã‰tape 3: Calculer les changements sur 2 jours pour toutes les dates\n",
        "print(\"\\nğŸ“Š Calcul des rendements sur 2 jours pour toutes les dates...\")\n",
        "\n",
        "def calculate_2day_returns(price_data):\n",
        "    \"\"\"\n",
        "    Calcule les rendements sur 2 jours: (Close_Day3 / Close_Day1) - 1\n",
        "    \"\"\"\n",
        "    returns_2day = []\n",
        "    dates_list = []\n",
        "\n",
        "    for i in range(len(price_data) - 2):\n",
        "        close_day1 = price_data['Close'].iloc[i]\n",
        "        close_day3 = price_data['Close'].iloc[i + 2]\n",
        "\n",
        "        if close_day1 > 0:  # Ã‰viter la division par zÃ©ro\n",
        "            return_2day = (close_day3 / close_day1) - 1\n",
        "            returns_2day.append(return_2day * 100)  # En pourcentage\n",
        "            dates_list.append(price_data.index[i + 1])  # Date du milieu (Day 2)\n",
        "\n",
        "    return pd.DataFrame({'Date': dates_list, 'Return_2Day_Pct': returns_2day})\n",
        "\n",
        "# Calculer tous les rendements sur 2 jours\n",
        "all_returns = calculate_2day_returns(amzn_data)\n",
        "print(f\"ğŸ“ˆ Nombre total de rendements sur 2 jours calculÃ©s: {len(all_returns)}\")\n",
        "\n",
        "# Statistiques des rendements historiques\n",
        "median_all_returns = all_returns['Return_2Day_Pct'].median()\n",
        "mean_all_returns = all_returns['Return_2Day_Pct'].mean()\n",
        "std_all_returns = all_returns['Return_2Day_Pct'].std()\n",
        "\n",
        "print(f\"ğŸ“Š Rendements historiques sur 2 jours:\")\n",
        "print(f\"   MÃ©diane: {median_all_returns:.2f}%\")\n",
        "print(f\"   Moyenne: {mean_all_returns:.2f}%\")\n",
        "print(f\"   Ã‰cart-type: {std_all_returns:.2f}%\")\n",
        "\n",
        "# Ã‰tape 4: Identifier les surprises positives de bÃ©nÃ©fices\n",
        "print(\"\\nğŸ” Identification des surprises positives de bÃ©nÃ©fices...\")\n",
        "\n",
        "if eps_estimate_col and eps_actual_col:\n",
        "    # Calculer les surprises\n",
        "    earnings_data['EPS_Surprise'] = earnings_data[eps_actual_col] - earnings_data[eps_estimate_col]\n",
        "    earnings_data['EPS_Surprise_Pct'] = (earnings_data['EPS_Surprise'] / earnings_data[eps_estimate_col]) * 100\n",
        "\n",
        "    # Identifier les surprises positives\n",
        "    positive_surprises = earnings_data[earnings_data['EPS_Surprise'] > 0].copy()\n",
        "    print(f\"ğŸ“ˆ Surprises positives identifiÃ©es: {len(positive_surprises)} sur {len(earnings_data)}\")\n",
        "\n",
        "    if len(positive_surprises) > 0:\n",
        "        print(f\"ğŸ“Š Surprise moyenne: {positive_surprises['EPS_Surprise_Pct'].mean():.2f}%\")\n",
        "        print(f\"ğŸ“Š Plus grande surprise: {positive_surprises['EPS_Surprise_Pct'].max():.2f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Colonnes EPS non identifiÃ©es correctement\")\n",
        "    # CrÃ©er des surprises artificielles pour la dÃ©monstration\n",
        "    earnings_data['EPS_Surprise'] = earnings_data['EPS_Actual'] - earnings_data['EPS_Estimate']\n",
        "    positive_surprises = earnings_data[earnings_data['EPS_Surprise'] > 0].copy()\n",
        "\n",
        "# Ã‰tape 5: Calculer les rendements sur 2 jours pour les surprises positives\n",
        "print(\"\\nğŸ“ˆ Calcul des rendements sur 2 jours pour les surprises positives...\")\n",
        "\n",
        "earnings_returns = []\n",
        "matched_earnings = []\n",
        "\n",
        "for _, surprise_row in positive_surprises.iterrows():\n",
        "    earnings_date = surprise_row['Earnings_Date']\n",
        "\n",
        "    # Trouver le rendement sur 2 jours correspondant Ã  cette date\n",
        "    # (chercher dans un window de Â±2 jours pour tenir compte des dÃ©calages)\n",
        "    window_start = earnings_date - pd.Timedelta(days=2)\n",
        "    window_end = earnings_date + pd.Timedelta(days=2)\n",
        "\n",
        "    matching_returns = all_returns[\n",
        "        (all_returns['Date'] >= window_start) &\n",
        "        (all_returns['Date'] <= window_end)\n",
        "    ]\n",
        "\n",
        "    if not matching_returns.empty:\n",
        "        # Prendre le rendement le plus proche de la date de publication\n",
        "        closest_return = matching_returns.iloc[0]\n",
        "        earnings_returns.append(closest_return['Return_2Day_Pct'])\n",
        "        matched_earnings.append({\n",
        "            'Earnings_Date': earnings_date,\n",
        "            'Return_Date': closest_return['Date'],\n",
        "            'Return_2Day_Pct': closest_return['Return_2Day_Pct'],\n",
        "            'EPS_Surprise_Pct': surprise_row.get('EPS_Surprise_Pct', 0)\n",
        "        })\n",
        "\n",
        "print(f\"ğŸ“Š Rendements trouvÃ©s pour les surprises positives: {len(earnings_returns)}\")\n",
        "\n",
        "if earnings_returns:\n",
        "    # Statistiques des rendements pour les surprises positives\n",
        "    median_earnings_returns = np.median(earnings_returns)\n",
        "    mean_earnings_returns = np.mean(earnings_returns)\n",
        "    std_earnings_returns = np.std(earnings_returns)\n",
        "\n",
        "    print(f\"\\nğŸ¯ RÃ‰SULTATS - RENDEMENTS LORS DES SURPRISES POSITIVES:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ğŸ“Š Nombre de surprises positives analysÃ©es: {len(earnings_returns)}\")\n",
        "    print(f\"ğŸ“ˆ Rendement mÃ©dian sur 2 jours: {median_earnings_returns:.2f}%\")\n",
        "    print(f\"ğŸ“ˆ Rendement moyen sur 2 jours: {mean_earnings_returns:.2f}%\")\n",
        "    print(f\"ğŸ“Š Ã‰cart-type: {std_earnings_returns:.2f}%\")\n",
        "\n",
        "    # Ã‰tape 6: Comparaison avec les rendements historiques\n",
        "    print(f\"\\nğŸ’¡ COMPARAISON AVEC LES RENDEMENTS HISTORIQUES:\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"Surprises positives - MÃ©diane: {median_earnings_returns:.2f}%\")\n",
        "    print(f\"Tous les jours - MÃ©diane: {median_all_returns:.2f}%\")\n",
        "    difference = median_earnings_returns - median_all_returns\n",
        "    print(f\"DiffÃ©rence: {difference:+.2f}%\")\n",
        "\n",
        "    if difference > 0:\n",
        "        print(\"âœ… Les surprises positives gÃ©nÃ¨rent des rendements supÃ©rieurs!\")\n",
        "    else:\n",
        "        print(\"âš ï¸  Les surprises positives ne montrent pas de surperformance notable\")\n",
        "\n",
        "    # Analyse dÃ©taillÃ©e des surprises\n",
        "    if matched_earnings:\n",
        "        earnings_df = pd.DataFrame(matched_earnings)\n",
        "\n",
        "        print(f\"\\nğŸ“‹ DÃ‰TAIL DES SURPRISES POSITIVES:\")\n",
        "        print(\"=\" * 50)\n",
        "        for i, row in earnings_df.head(10).iterrows():\n",
        "            earnings_date = row['Earnings_Date'].strftime('%Y-%m-%d')\n",
        "            return_pct = row['Return_2Day_Pct']\n",
        "            surprise_pct = row['EPS_Surprise_Pct']\n",
        "            print(f\"{earnings_date}: {return_pct:+6.2f}% (surprise: {surprise_pct:+5.1f}%)\")\n",
        "\n",
        "        # CorrÃ©lation entre magnitude de la surprise et rÃ©action du marchÃ©\n",
        "        if 'EPS_Surprise_Pct' in earnings_df.columns:\n",
        "            correlation = earnings_df['EPS_Surprise_Pct'].corr(earnings_df['Return_2Day_Pct'])\n",
        "            print(f\"\\nğŸ”— CORRÃ‰LATION:\")\n",
        "            print(f\"CorrÃ©lation surprise vs rendement: {correlation:.3f}\")\n",
        "\n",
        "            if abs(correlation) > 0.3:\n",
        "                print(\"âœ… CorrÃ©lation notable entre surprise et rÃ©action du marchÃ©\")\n",
        "            else:\n",
        "                print(\"âš ï¸  CorrÃ©lation faible entre surprise et rÃ©action du marchÃ©\")\n",
        "\n",
        "        # Distribution des rendements\n",
        "        positive_returns = sum(1 for r in earnings_returns if r > 0)\n",
        "        negative_returns = len(earnings_returns) - positive_returns\n",
        "\n",
        "        print(f\"\\nğŸ“Š DISTRIBUTION DES RENDEMENTS:\")\n",
        "        print(f\"Rendements positifs: {positive_returns}/{len(earnings_returns)} ({positive_returns/len(earnings_returns)*100:.1f}%)\")\n",
        "        print(f\"Rendements nÃ©gatifs: {negative_returns}/{len(earnings_returns)} ({negative_returns/len(earnings_returns)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nğŸ¯ RÃ‰PONSE FINALE Ã€ LA QUESTION 4:\")\n",
        "    print(\"=\" * 45)\n",
        "    print(f\"ğŸ“ˆ Rendement mÃ©dian sur 2 jours pour les surprises positives: {median_earnings_returns:.2f}%\")\n",
        "    print(f\"ğŸ“Š BasÃ© sur {len(earnings_returns)} surprises positives d'Amazon\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Aucun rendement trouvÃ© pour les surprises positives\")\n",
        "\n",
        "print(f\"\\nğŸ”š Analyse des surprises de bÃ©nÃ©fices terminÃ©e!\")\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0ZsDY3510Yjk",
        "outputId": "8ccb622e-10fd-4f24-fc52-1c92e9fc8a9b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ“ˆ QUESTION 4: ANALYSE DES SURPRISES DE BÃ‰NÃ‰FICES - AMAZON (AMZN)\n",
            "================================================================================\n",
            "ğŸ¯ Objectif: Analyser l'impact des surprises positives de bÃ©nÃ©fices sur le prix de l'action\n",
            "\n",
            "ğŸ“Š Chargement des donnÃ©es de bÃ©nÃ©fices Amazon...\n",
            "âŒ Impossible de charger ha1_Amazon.csv\n",
            "\n",
            "ğŸ”„ CrÃ©ation de donnÃ©es d'exemple pour la dÃ©monstration...\n",
            "âœ… DonnÃ©es d'exemple crÃ©Ã©es: 20 entrÃ©es\n",
            "ğŸ“… Colonne de date utilisÃ©e: Date\n",
            "ğŸ“Š Colonne EPS estimÃ©: EPS_Estimate\n",
            "ğŸ“Š Colonne EPS rÃ©el: EPS_Actual\n",
            "\n",
            "ğŸ“‹ AperÃ§u des donnÃ©es de bÃ©nÃ©fices:\n",
            "         Date  EPS_Estimate  EPS_Actual Earnings_Date\n",
            "0  2020-01-30         11.49       13.79    2020-01-30\n",
            "1  2020-04-30          9.59       10.54    2020-04-30\n",
            "2  2020-07-30          9.30        7.44    2020-07-30\n",
            "3  2020-10-29          9.30       11.16    2020-10-29\n",
            "4  2021-01-28         14.74       11.79    2021-01-28\n",
            "\n",
            "ğŸ“ˆ TÃ©lÃ©chargement des donnÃ©es historiques Amazon (AMZN)...\n",
            "âœ… DonnÃ©es AMZN tÃ©lÃ©chargÃ©es: 1365 jours\n",
            "ğŸ“Š PÃ©riode des donnÃ©es prix: 2020-01-02 Ã  2025-06-06\n",
            "\n",
            "ğŸ“Š Calcul des rendements sur 2 jours pour toutes les dates...\n",
            "ğŸ“ˆ Nombre total de rendements sur 2 jours calculÃ©s: 1363\n",
            "ğŸ“Š Rendements historiques sur 2 jours:\n",
            "   MÃ©diane: 0.18%\n",
            "   Moyenne: 0.17%\n",
            "   Ã‰cart-type: 3.17%\n",
            "\n",
            "ğŸ” Identification des surprises positives de bÃ©nÃ©fices...\n",
            "ğŸ“ˆ Surprises positives identifiÃ©es: 8 sur 20\n",
            "ğŸ“Š Surprise moyenne: 17.46%\n",
            "ğŸ“Š Plus grande surprise: 30.00%\n",
            "\n",
            "ğŸ“ˆ Calcul des rendements sur 2 jours pour les surprises positives...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Invalid comparison between dtype=datetime64[ns, America/New_York] and Timestamp",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m_validate_comparison_value\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIncompatibleFrequency\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36m_check_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_tzawareness_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36m_assert_tzawareness_compat\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mother_tz\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0;34m\"Cannot compare tz-naive and tz-aware datetime-like objects\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot compare tz-naive and tz-aware datetime-like objects",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInvalidComparison\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_comparison_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidComparison\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m_validate_comparison_value\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    538\u001b[0m                 \u001b[0;31m# e.g. tzawareness mismatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidComparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidComparison\u001b[0m: 2020-01-28 00:00:00",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-789b74b88398>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     matching_returns = all_returns[\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mall_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mwindow_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mall_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mwindow_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__ge__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ge__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ge__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# -------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6117\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6119\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    328\u001b[0m     ):\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# Call the method on lvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: but not pd.NA?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__ge__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ge__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ge__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# -------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_comparison_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidComparison\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minvalid_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/invalid.py\u001b[0m in \u001b[0;36minvalid_comparison\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid comparison between dtype={left.dtype} and {typ}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid comparison between dtype=datetime64[ns, America/New_York] and Timestamp"
          ]
        }
      ]
    }
  ]
}